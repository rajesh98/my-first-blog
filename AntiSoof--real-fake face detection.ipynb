{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOIfNFW6spl3nsSrTpQHLpY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajesh98/my-first-blog/blob/master/AntiSoof--real-fake%20face%20detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oarTI9kP3P3X",
        "outputId": "6d5f594a-66b9-46d2-90ee-9afacaddba9b"
      },
      "source": [
        "import os\n",
        "print(os.getcwd())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZeuv6Al4_e1"
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from scipy import ndimage"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNTlpV1q5FSx"
      },
      "source": [
        "net = cv2.dnn.readNetFromCaffe('/content/sample_data/deploy.prototxt.txt', '/content/sample_data/openCVfaceDetectorModel1.caffemodel')"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8ZQxn_K6eHm"
      },
      "source": [
        "vs = cv2.VideoCapture('/content/sample_data/MakePeopleSmileProject.mp4')"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CI-D0H3gInbM"
      },
      "source": [
        "vs2 = cv2.VideoCapture('/content/sample_data/MakePeopleSmileProjectFAKE.mp4')"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQdlE3mR6rKA"
      },
      "source": [
        "read = 0\n",
        "saved = 0"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNdUW49e6yW9",
        "outputId": "f02b40a8-0568-47c1-825b-67edd61e6649"
      },
      "source": [
        "while True:\n",
        "  #print(\"start\")\n",
        "  (grabbed,frame) = vs2.read()\n",
        "  if not grabbed:\n",
        "    break\n",
        "  read = read+1\n",
        "  if read%9 !=0:\n",
        "    continue\n",
        "  (h,w) = frame.shape[:2]\n",
        "  blob = cv2.dnn.blobFromImage(cv2.resize(frame,(300,300)),1.0,(300,300),(104.0,177.0,123.0))\n",
        "  net.setInput(blob)\n",
        "  detections = net.forward()\n",
        "  #print(len(detections))\n",
        "  if len(detections>0):\n",
        "    i = np.argmax(detections[0,0,:,2])\n",
        "    confidence = detections[0,0,i,2]\n",
        "    print(confidence)\n",
        "    if confidence>0.5:\n",
        "      box = detections[0,0,i,3:7] * np.array([w,h,w,h])\n",
        "      (startX,startY,endX,endY) = box.astype(\"int\")\n",
        "      face = frame[startY:endY, startX:endX]\n",
        "      rotated = ndimage.rotate(face, -90)\n",
        "      op = os.path.sep.join(['/content/sample_data/dataset/fake', \"{}.png\".format(saved)])\n",
        "      try:\n",
        "        cv2.imwrite(op,rotated)\n",
        "      except:\n",
        "        print(\"error\")\n",
        "\n",
        "      saved = saved+1\n"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.52433187\n",
            "0.6790611\n",
            "0.43499672\n",
            "0.39483902\n",
            "0.3060459\n",
            "0.26415727\n",
            "0.94581294\n",
            "0.87593246\n",
            "0.9603117\n",
            "0.9565402\n",
            "0.9675134\n",
            "0.9055092\n",
            "0.94372505\n",
            "0.9617667\n",
            "0.95012295\n",
            "0.9555995\n",
            "0.9709692\n",
            "0.96416336\n",
            "0.9832548\n",
            "0.46959525\n",
            "0.37060103\n",
            "0.58204985\n",
            "0.57881564\n",
            "0.679513\n",
            "0.6277932\n",
            "0.2077564\n",
            "0.22706257\n",
            "0.2442976\n",
            "0.22100902\n",
            "0.2470604\n",
            "0.27067387\n",
            "0.17740081\n",
            "0.99643207\n",
            "0.9899267\n",
            "0.95620275\n",
            "0.9922174\n",
            "0.97621626\n",
            "0.99216014\n",
            "0.97827303\n",
            "0.33513165\n",
            "0.4070788\n",
            "0.39042202\n",
            "0.32429826\n",
            "0.14586538\n",
            "0.30099565\n",
            "0.36289728\n",
            "0.5972153\n",
            "0.6262619\n",
            "0.56985086\n",
            "0.62462825\n",
            "0.7214305\n",
            "0.58641404\n",
            "0.77624\n",
            "0.58214855\n",
            "0.47721544\n",
            "0.48350617\n",
            "0.13278697\n",
            "0.13191596\n",
            "0.14345072\n",
            "0.1326012\n",
            "0.13697173\n",
            "0.14592086\n",
            "0.13716486\n",
            "0.5147828\n",
            "0.6039734\n",
            "0.6422162\n",
            "0.6056104\n",
            "0.6456888\n",
            "0.7044872\n",
            "0.8579336\n",
            "0.744902\n",
            "0.77613974\n",
            "0.77381855\n",
            "0.77732563\n",
            "0.5890916\n",
            "0.13440141\n",
            "0.13614292\n",
            "0.13750128\n",
            "0.13639861\n",
            "0.13641576\n",
            "0.1353547\n",
            "0.13659684\n",
            "0.13782543\n",
            "0.1401389\n",
            "0.13629048\n",
            "0.13805276\n",
            "0.13648194\n",
            "0.13657515\n",
            "0.14234301\n",
            "0.19346127\n",
            "0.24556749\n",
            "0.3652567\n",
            "0.27544442\n",
            "0.5523934\n",
            "0.6633536\n",
            "0.78357077\n",
            "0.9159568\n",
            "0.8985461\n",
            "0.14964737\n",
            "0.17929554\n",
            "0.14725839\n",
            "0.99074805\n",
            "0.9506304\n",
            "0.9245206\n",
            "0.8391875\n",
            "0.42297125\n",
            "0.837137\n",
            "0.91118926\n",
            "0.9197771\n",
            "0.8510424\n",
            "0.88012236\n",
            "0.76364315\n",
            "0.61039305\n",
            "0.6425223\n",
            "0.6480937\n",
            "0.778906\n",
            "0.57273155\n",
            "0.37920734\n",
            "0.37074265\n",
            "0.3074327\n",
            "0.88052505\n",
            "0.98078257\n",
            "0.16302554\n",
            "0.1473029\n",
            "0.19545412\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LI9Xxk35-Nyb",
        "outputId": "f91378f5-a2cf-4cb4-8ce5-8a832f97a246"
      },
      "source": [
        "print(read)\n",
        "print(saved)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1126\n",
            "68\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QxoFFKvPw5t"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras import backend as K"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yGPrHPo3Srr"
      },
      "source": [
        "class LivenessNet:\n",
        "\t@staticmethod\n",
        "\tdef build(width, height, depth, classes):\n",
        "\t\t# initialize the model along with the input shape to be\n",
        "\t\t# \"channels last\" and the channels dimension itself\n",
        "\t\tmodel = Sequential()\n",
        "\t\tinputShape = (height, width, depth)\n",
        "\t\tchanDim = -1\n",
        "\t\t# if we are using \"channels first\", update the input shape\n",
        "\t\t# and channels dimension\n",
        "\t\tif K.image_data_format() == \"channels_first\":\n",
        "\t\t\tinputShape = (depth, height, width)\n",
        "\t\t\tchanDim = 1\n",
        "\n",
        "        # first CONV => RELU => CONV => RELU => POOL layer set\n",
        "\t\tmodel.add(Conv2D(16, (3, 3), padding=\"same\",\n",
        "\t\t\tinput_shape=inputShape))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(Conv2D(16, (3, 3), padding=\"same\"))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\t\tmodel.add(Dropout(0.25))\n",
        "\n",
        "        # second CONV => RELU => CONV => RELU => POOL layer set\n",
        "\t\tmodel.add(Conv2D(32, (3, 3), padding=\"same\"))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(Conv2D(32, (3, 3), padding=\"same\"))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\t\tmodel.add(Dropout(0.25))\n",
        "\n",
        "        # first (and only) set of FC => RELU layers\n",
        "\t\tmodel.add(Flatten())\n",
        "\t\tmodel.add(Dense(64))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization())\n",
        "\t\tmodel.add(Dropout(0.5))\n",
        "\t\t# softmax classifier\n",
        "\t\tmodel.add(Dense(classes))\n",
        "\t\tmodel.add(Activation(\"softmax\"))\n",
        "\n",
        "        # return the constructed network architecture\n",
        "\t\treturn model"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtJi7WP13XE2"
      },
      "source": [
        "# set the matplotlib backend so figures can be saved in the background\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "# import the necessary packages\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from imutils import paths\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "import pickle\n",
        "import cv2\n",
        "import os"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd3kAEBZ30CN",
        "outputId": "d4ce4fa8-2768-40f4-b8d0-e84d479f6bb8"
      },
      "source": [
        "# initialize the initial learning rate, batch size, and number of\n",
        "# epochs to train for\n",
        "INIT_LR = 1e-4\n",
        "BS = 8\n",
        "EPOCHS = 50\n",
        "# grab the list of images in our dataset directory, then initialize\n",
        "# the list of data (i.e., images) and class images\n",
        "print(\"[INFO] loading images...\")\n",
        "imagePaths = list(paths.list_images('/content/sample_data/dataset'))\n",
        "data = []\n",
        "labels = []\n",
        "# loop over all image paths\n",
        "for imagePath in imagePaths:\n",
        "\t# extract the class label from the filename, load the image and\n",
        "\t# resize it to be a fixed 32x32 pixels, ignoring aspect ratio\n",
        "  print(imagePath)\n",
        "  label = imagePath.split(os.path.sep)[-2]\n",
        "  print(label)\n",
        "  image = cv2.imread(imagePath)\n",
        "  image = cv2.resize(image, (32, 32))\n",
        "\t# update the data and labels lists, respectively\n",
        "  data.append(image)\n",
        "  labels.append(label)\n",
        "# convert the data into a NumPy array, then preprocess it by scaling\n",
        "# all pixel intensities to the range [0, 1]\n",
        "data = np.array(data, dtype=\"float\") / 255.0"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] loading images...\n",
            "/content/sample_data/dataset/real/172.png\n",
            "real\n",
            "/content/sample_data/dataset/real/26.png\n",
            "real\n",
            "/content/sample_data/dataset/real/27.png\n",
            "real\n",
            "/content/sample_data/dataset/real/127.png\n",
            "real\n",
            "/content/sample_data/dataset/real/73.png\n",
            "real\n",
            "/content/sample_data/dataset/real/98.png\n",
            "real\n",
            "/content/sample_data/dataset/real/14.png\n",
            "real\n",
            "/content/sample_data/dataset/real/44.png\n",
            "real\n",
            "/content/sample_data/dataset/real/168.png\n",
            "real\n",
            "/content/sample_data/dataset/real/23.png\n",
            "real\n",
            "/content/sample_data/dataset/real/60.png\n",
            "real\n",
            "/content/sample_data/dataset/real/126.png\n",
            "real\n",
            "/content/sample_data/dataset/real/79.png\n",
            "real\n",
            "/content/sample_data/dataset/real/137.png\n",
            "real\n",
            "/content/sample_data/dataset/real/158.png\n",
            "real\n",
            "/content/sample_data/dataset/real/24.png\n",
            "real\n",
            "/content/sample_data/dataset/real/10.png\n",
            "real\n",
            "/content/sample_data/dataset/real/153.png\n",
            "real\n",
            "/content/sample_data/dataset/real/130.png\n",
            "real\n",
            "/content/sample_data/dataset/real/181.png\n",
            "real\n",
            "/content/sample_data/dataset/real/83.png\n",
            "real\n",
            "/content/sample_data/dataset/real/64.png\n",
            "real\n",
            "/content/sample_data/dataset/real/12.png\n",
            "real\n",
            "/content/sample_data/dataset/real/97.png\n",
            "real\n",
            "/content/sample_data/dataset/real/173.png\n",
            "real\n",
            "/content/sample_data/dataset/real/0.png\n",
            "real\n",
            "/content/sample_data/dataset/real/117.png\n",
            "real\n",
            "/content/sample_data/dataset/real/118.png\n",
            "real\n",
            "/content/sample_data/dataset/real/174.png\n",
            "real\n",
            "/content/sample_data/dataset/real/39.png\n",
            "real\n",
            "/content/sample_data/dataset/real/140.png\n",
            "real\n",
            "/content/sample_data/dataset/real/59.png\n",
            "real\n",
            "/content/sample_data/dataset/real/132.png\n",
            "real\n",
            "/content/sample_data/dataset/real/135.png\n",
            "real\n",
            "/content/sample_data/dataset/real/80.png\n",
            "real\n",
            "/content/sample_data/dataset/real/110.png\n",
            "real\n",
            "/content/sample_data/dataset/real/43.png\n",
            "real\n",
            "/content/sample_data/dataset/real/13.png\n",
            "real\n",
            "/content/sample_data/dataset/real/179.png\n",
            "real\n",
            "/content/sample_data/dataset/real/85.png\n",
            "real\n",
            "/content/sample_data/dataset/real/86.png\n",
            "real\n",
            "/content/sample_data/dataset/real/128.png\n",
            "real\n",
            "/content/sample_data/dataset/real/151.png\n",
            "real\n",
            "/content/sample_data/dataset/real/65.png\n",
            "real\n",
            "/content/sample_data/dataset/real/161.png\n",
            "real\n",
            "/content/sample_data/dataset/real/149.png\n",
            "real\n",
            "/content/sample_data/dataset/real/54.png\n",
            "real\n",
            "/content/sample_data/dataset/real/19.png\n",
            "real\n",
            "/content/sample_data/dataset/real/82.png\n",
            "real\n",
            "/content/sample_data/dataset/real/50.png\n",
            "real\n",
            "/content/sample_data/dataset/real/106.png\n",
            "real\n",
            "/content/sample_data/dataset/real/49.png\n",
            "real\n",
            "/content/sample_data/dataset/real/9.png\n",
            "real\n",
            "/content/sample_data/dataset/real/111.png\n",
            "real\n",
            "/content/sample_data/dataset/real/177.png\n",
            "real\n",
            "/content/sample_data/dataset/real/3.png\n",
            "real\n",
            "/content/sample_data/dataset/real/5.png\n",
            "real\n",
            "/content/sample_data/dataset/real/159.png\n",
            "real\n",
            "/content/sample_data/dataset/real/185.png\n",
            "real\n",
            "/content/sample_data/dataset/real/164.png\n",
            "real\n",
            "/content/sample_data/dataset/real/55.png\n",
            "real\n",
            "/content/sample_data/dataset/real/101.png\n",
            "real\n",
            "/content/sample_data/dataset/real/108.png\n",
            "real\n",
            "/content/sample_data/dataset/real/37.png\n",
            "real\n",
            "/content/sample_data/dataset/real/4.png\n",
            "real\n",
            "/content/sample_data/dataset/real/156.png\n",
            "real\n",
            "/content/sample_data/dataset/real/78.png\n",
            "real\n",
            "/content/sample_data/dataset/real/77.png\n",
            "real\n",
            "/content/sample_data/dataset/real/51.png\n",
            "real\n",
            "/content/sample_data/dataset/real/167.png\n",
            "real\n",
            "/content/sample_data/dataset/real/178.png\n",
            "real\n",
            "/content/sample_data/dataset/real/125.png\n",
            "real\n",
            "/content/sample_data/dataset/real/38.png\n",
            "real\n",
            "/content/sample_data/dataset/real/170.png\n",
            "real\n",
            "/content/sample_data/dataset/real/72.png\n",
            "real\n",
            "/content/sample_data/dataset/real/148.png\n",
            "real\n",
            "/content/sample_data/dataset/real/145.png\n",
            "real\n",
            "/content/sample_data/dataset/real/147.png\n",
            "real\n",
            "/content/sample_data/dataset/real/112.png\n",
            "real\n",
            "/content/sample_data/dataset/real/103.png\n",
            "real\n",
            "/content/sample_data/dataset/real/139.png\n",
            "real\n",
            "/content/sample_data/dataset/real/90.png\n",
            "real\n",
            "/content/sample_data/dataset/real/21.png\n",
            "real\n",
            "/content/sample_data/dataset/real/157.png\n",
            "real\n",
            "/content/sample_data/dataset/real/175.png\n",
            "real\n",
            "/content/sample_data/dataset/real/84.png\n",
            "real\n",
            "/content/sample_data/dataset/real/2.png\n",
            "real\n",
            "/content/sample_data/dataset/real/116.png\n",
            "real\n",
            "/content/sample_data/dataset/real/113.png\n",
            "real\n",
            "/content/sample_data/dataset/real/96.png\n",
            "real\n",
            "/content/sample_data/dataset/real/18.png\n",
            "real\n",
            "/content/sample_data/dataset/real/28.png\n",
            "real\n",
            "/content/sample_data/dataset/real/81.png\n",
            "real\n",
            "/content/sample_data/dataset/real/41.png\n",
            "real\n",
            "/content/sample_data/dataset/real/150.png\n",
            "real\n",
            "/content/sample_data/dataset/real/129.png\n",
            "real\n",
            "/content/sample_data/dataset/real/74.png\n",
            "real\n",
            "/content/sample_data/dataset/real/56.png\n",
            "real\n",
            "/content/sample_data/dataset/real/52.png\n",
            "real\n",
            "/content/sample_data/dataset/real/184.png\n",
            "real\n",
            "/content/sample_data/dataset/real/22.png\n",
            "real\n",
            "/content/sample_data/dataset/real/95.png\n",
            "real\n",
            "/content/sample_data/dataset/real/134.png\n",
            "real\n",
            "/content/sample_data/dataset/real/162.png\n",
            "real\n",
            "/content/sample_data/dataset/real/138.png\n",
            "real\n",
            "/content/sample_data/dataset/real/166.png\n",
            "real\n",
            "/content/sample_data/dataset/real/187.png\n",
            "real\n",
            "/content/sample_data/dataset/real/160.png\n",
            "real\n",
            "/content/sample_data/dataset/real/131.png\n",
            "real\n",
            "/content/sample_data/dataset/real/75.png\n",
            "real\n",
            "/content/sample_data/dataset/real/58.png\n",
            "real\n",
            "/content/sample_data/dataset/real/146.png\n",
            "real\n",
            "/content/sample_data/dataset/real/62.png\n",
            "real\n",
            "/content/sample_data/dataset/real/16.png\n",
            "real\n",
            "/content/sample_data/dataset/real/182.png\n",
            "real\n",
            "/content/sample_data/dataset/real/107.png\n",
            "real\n",
            "/content/sample_data/dataset/real/67.png\n",
            "real\n",
            "/content/sample_data/dataset/real/69.png\n",
            "real\n",
            "/content/sample_data/dataset/real/88.png\n",
            "real\n",
            "/content/sample_data/dataset/real/190.png\n",
            "real\n",
            "/content/sample_data/dataset/real/8.png\n",
            "real\n",
            "/content/sample_data/dataset/real/165.png\n",
            "real\n",
            "/content/sample_data/dataset/real/1.png\n",
            "real\n",
            "/content/sample_data/dataset/real/45.png\n",
            "real\n",
            "/content/sample_data/dataset/real/15.png\n",
            "real\n",
            "/content/sample_data/dataset/real/105.png\n",
            "real\n",
            "/content/sample_data/dataset/real/183.png\n",
            "real\n",
            "/content/sample_data/dataset/real/11.png\n",
            "real\n",
            "/content/sample_data/dataset/real/57.png\n",
            "real\n",
            "/content/sample_data/dataset/real/114.png\n",
            "real\n",
            "/content/sample_data/dataset/real/176.png\n",
            "real\n",
            "/content/sample_data/dataset/real/17.png\n",
            "real\n",
            "/content/sample_data/dataset/real/109.png\n",
            "real\n",
            "/content/sample_data/dataset/real/100.png\n",
            "real\n",
            "/content/sample_data/dataset/real/188.png\n",
            "real\n",
            "/content/sample_data/dataset/real/76.png\n",
            "real\n",
            "/content/sample_data/dataset/real/61.png\n",
            "real\n",
            "/content/sample_data/dataset/real/163.png\n",
            "real\n",
            "/content/sample_data/dataset/real/189.png\n",
            "real\n",
            "/content/sample_data/dataset/real/143.png\n",
            "real\n",
            "/content/sample_data/dataset/real/154.png\n",
            "real\n",
            "/content/sample_data/dataset/real/40.png\n",
            "real\n",
            "/content/sample_data/dataset/real/68.png\n",
            "real\n",
            "/content/sample_data/dataset/real/42.png\n",
            "real\n",
            "/content/sample_data/dataset/real/63.png\n",
            "real\n",
            "/content/sample_data/dataset/real/87.png\n",
            "real\n",
            "/content/sample_data/dataset/real/47.png\n",
            "real\n",
            "/content/sample_data/dataset/real/171.png\n",
            "real\n",
            "/content/sample_data/dataset/real/155.png\n",
            "real\n",
            "/content/sample_data/dataset/real/25.png\n",
            "real\n",
            "/content/sample_data/dataset/real/180.png\n",
            "real\n",
            "/content/sample_data/dataset/real/142.png\n",
            "real\n",
            "/content/sample_data/dataset/real/46.png\n",
            "real\n",
            "/content/sample_data/dataset/real/99.png\n",
            "real\n",
            "/content/sample_data/dataset/real/6.png\n",
            "real\n",
            "/content/sample_data/dataset/real/102.png\n",
            "real\n",
            "/content/sample_data/dataset/real/53.png\n",
            "real\n",
            "/content/sample_data/dataset/real/89.png\n",
            "real\n",
            "/content/sample_data/dataset/real/20.png\n",
            "real\n",
            "/content/sample_data/dataset/real/7.png\n",
            "real\n",
            "/content/sample_data/dataset/real/29.png\n",
            "real\n",
            "/content/sample_data/dataset/real/70.png\n",
            "real\n",
            "/content/sample_data/dataset/real/136.png\n",
            "real\n",
            "/content/sample_data/dataset/real/66.png\n",
            "real\n",
            "/content/sample_data/dataset/real/48.png\n",
            "real\n",
            "/content/sample_data/dataset/real/71.png\n",
            "real\n",
            "/content/sample_data/dataset/real/186.png\n",
            "real\n",
            "/content/sample_data/dataset/real/115.png\n",
            "real\n",
            "/content/sample_data/dataset/real/169.png\n",
            "real\n",
            "/content/sample_data/dataset/real/141.png\n",
            "real\n",
            "/content/sample_data/dataset/real/152.png\n",
            "real\n",
            "/content/sample_data/dataset/real/144.png\n",
            "real\n",
            "/content/sample_data/dataset/fake/30.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/26.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/27.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/14.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/44.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/23.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/60.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/24.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/10.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/64.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/12.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/0.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/39.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/59.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/43.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/13.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/65.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/34.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/32.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/54.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/19.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/50.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/49.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/9.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/3.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/5.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/55.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/37.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/4.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/51.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/38.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/21.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/2.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/18.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/28.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/41.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/56.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/52.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/33.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/22.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/36.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/58.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/62.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/16.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/67.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/35.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/8.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/1.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/45.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/15.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/11.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/57.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/31.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/17.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/61.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/40.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/42.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/63.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/47.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/25.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/46.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/6.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/53.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/20.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/7.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/29.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/66.png\n",
            "fake\n",
            "/content/sample_data/dataset/fake/48.png\n",
            "fake\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFs1AN1S9Uze"
      },
      "source": [
        "# encode the labels (which are currently strings) as integers and then\n",
        "# one-hot encode them\n",
        "le = LabelEncoder()\n",
        "labels = le.fit_transform(labels)\n",
        "labels = to_categorical(labels, 2)\n",
        "# partition the data into training and testing splits using 75% of\n",
        "# the data for training and the remaining 25% for testing\n",
        "(trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
        "\ttest_size=0.25, random_state=42)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahJaNUoQ9Ew5",
        "outputId": "623f4799-9328-4ea7-918b-7fc3be84a5b0"
      },
      "source": [
        "print(len(data))"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFggiusf9xnE",
        "outputId": "841c4760-467e-4b7f-835f-585d77fbbd01"
      },
      "source": [
        "# construct the training image generator for data augmentation\n",
        "aug = ImageDataGenerator(rotation_range=20, zoom_range=0.15,\n",
        "\twidth_shift_range=0.2, height_shift_range=0.2, shear_range=0.15,\n",
        "\thorizontal_flip=True, fill_mode=\"nearest\")\n",
        "# initialize the optimizer and model\n",
        "print(\"[INFO] compiling model...\")\n",
        "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
        "model = LivenessNet.build(width=32, height=32, depth=3,\n",
        "\tclasses=len(le.classes_))\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])\n",
        "# train the network\n",
        "print(\"[INFO] training network for {} epochs...\".format(EPOCHS))\n",
        "H = model.fit(x=aug.flow(trainX, trainY, batch_size=BS),\n",
        "\tvalidation_data=(testX, testY), steps_per_epoch=len(trainX) // BS,\n",
        "\tepochs=EPOCHS)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] compiling model...\n",
            "[INFO] training network for 50 epochs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "22/22 [==============================] - 2s 46ms/step - loss: 0.9449 - accuracy: 0.5581 - val_loss: 0.6666 - val_accuracy: 0.6333\n",
            "Epoch 2/50\n",
            "22/22 [==============================] - 1s 34ms/step - loss: 0.6975 - accuracy: 0.7442 - val_loss: 0.6450 - val_accuracy: 0.6333\n",
            "Epoch 3/50\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 0.5929 - accuracy: 0.7907 - val_loss: 0.6423 - val_accuracy: 0.6333\n",
            "Epoch 4/50\n",
            "22/22 [==============================] - 1s 33ms/step - loss: 0.5380 - accuracy: 0.7791 - val_loss: 0.6444 - val_accuracy: 0.6333\n",
            "Epoch 5/50\n",
            "22/22 [==============================] - 1s 34ms/step - loss: 0.4917 - accuracy: 0.8314 - val_loss: 0.6429 - val_accuracy: 0.6333\n",
            "Epoch 6/50\n",
            "22/22 [==============================] - 1s 36ms/step - loss: 0.4695 - accuracy: 0.8198 - val_loss: 0.6492 - val_accuracy: 0.6833\n",
            "Epoch 7/50\n",
            "22/22 [==============================] - 1s 36ms/step - loss: 0.4322 - accuracy: 0.8488 - val_loss: 0.6416 - val_accuracy: 0.7667\n",
            "Epoch 8/50\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 0.4038 - accuracy: 0.8663 - val_loss: 0.6337 - val_accuracy: 0.9667\n",
            "Epoch 9/50\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 0.4282 - accuracy: 0.8198 - val_loss: 0.6052 - val_accuracy: 1.0000\n",
            "Epoch 10/50\n",
            "22/22 [==============================] - 1s 36ms/step - loss: 0.3872 - accuracy: 0.8721 - val_loss: 0.6045 - val_accuracy: 0.9167\n",
            "Epoch 11/50\n",
            "22/22 [==============================] - 1s 37ms/step - loss: 0.2877 - accuracy: 0.9012 - val_loss: 0.6036 - val_accuracy: 0.8333\n",
            "Epoch 12/50\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 0.3026 - accuracy: 0.8895 - val_loss: 0.5982 - val_accuracy: 0.7667\n",
            "Epoch 13/50\n",
            "22/22 [==============================] - 1s 36ms/step - loss: 0.3081 - accuracy: 0.9128 - val_loss: 0.6172 - val_accuracy: 0.6000\n",
            "Epoch 14/50\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 0.2815 - accuracy: 0.9012 - val_loss: 0.5819 - val_accuracy: 0.7167\n",
            "Epoch 15/50\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 0.3056 - accuracy: 0.9128 - val_loss: 0.5614 - val_accuracy: 0.7000\n",
            "Epoch 16/50\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 0.2687 - accuracy: 0.9186 - val_loss: 0.5414 - val_accuracy: 0.7167\n",
            "Epoch 17/50\n",
            "22/22 [==============================] - 1s 34ms/step - loss: 0.2548 - accuracy: 0.9186 - val_loss: 0.4731 - val_accuracy: 0.7667\n",
            "Epoch 18/50\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 0.2483 - accuracy: 0.9244 - val_loss: 0.4131 - val_accuracy: 0.8333\n",
            "Epoch 19/50\n",
            "22/22 [==============================] - 1s 34ms/step - loss: 0.2067 - accuracy: 0.9360 - val_loss: 0.3661 - val_accuracy: 0.8500\n",
            "Epoch 20/50\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 0.3343 - accuracy: 0.8953 - val_loss: 0.3471 - val_accuracy: 0.8667\n",
            "Epoch 21/50\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 0.2447 - accuracy: 0.9205 - val_loss: 0.3302 - val_accuracy: 0.9000\n",
            "Epoch 22/50\n",
            "22/22 [==============================] - 1s 34ms/step - loss: 0.2033 - accuracy: 0.9419 - val_loss: 0.2643 - val_accuracy: 0.9500\n",
            "Epoch 23/50\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 0.2112 - accuracy: 0.9477 - val_loss: 0.2388 - val_accuracy: 0.9500\n",
            "Epoch 24/50\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 0.2094 - accuracy: 0.9302 - val_loss: 0.2259 - val_accuracy: 0.9500\n",
            "Epoch 25/50\n",
            "22/22 [==============================] - 1s 36ms/step - loss: 0.2798 - accuracy: 0.9186 - val_loss: 0.2127 - val_accuracy: 0.9833\n",
            "Epoch 26/50\n",
            "22/22 [==============================] - 1s 32ms/step - loss: 0.2757 - accuracy: 0.9244 - val_loss: 0.1823 - val_accuracy: 0.9833\n",
            "Epoch 27/50\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 0.1898 - accuracy: 0.9302 - val_loss: 0.1625 - val_accuracy: 1.0000\n",
            "Epoch 28/50\n",
            "22/22 [==============================] - 1s 36ms/step - loss: 0.2484 - accuracy: 0.9302 - val_loss: 0.1529 - val_accuracy: 0.9667\n",
            "Epoch 29/50\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 0.2493 - accuracy: 0.9419 - val_loss: 0.1347 - val_accuracy: 1.0000\n",
            "Epoch 30/50\n",
            "22/22 [==============================] - 1s 36ms/step - loss: 0.2551 - accuracy: 0.9318 - val_loss: 0.1224 - val_accuracy: 1.0000\n",
            "Epoch 31/50\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 0.1690 - accuracy: 0.9419 - val_loss: 0.1036 - val_accuracy: 1.0000\n",
            "Epoch 32/50\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 0.2173 - accuracy: 0.9419 - val_loss: 0.0899 - val_accuracy: 1.0000\n",
            "Epoch 33/50\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 0.3481 - accuracy: 0.8920 - val_loss: 0.0741 - val_accuracy: 1.0000\n",
            "Epoch 34/50\n",
            "22/22 [==============================] - 1s 36ms/step - loss: 0.2667 - accuracy: 0.9070 - val_loss: 0.0640 - val_accuracy: 1.0000\n",
            "Epoch 35/50\n",
            "22/22 [==============================] - 1s 36ms/step - loss: 0.1182 - accuracy: 0.9709 - val_loss: 0.0706 - val_accuracy: 1.0000\n",
            "Epoch 36/50\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 0.2170 - accuracy: 0.9360 - val_loss: 0.0642 - val_accuracy: 1.0000\n",
            "Epoch 37/50\n",
            "22/22 [==============================] - 1s 34ms/step - loss: 0.2252 - accuracy: 0.9419 - val_loss: 0.0687 - val_accuracy: 1.0000\n",
            "Epoch 38/50\n",
            "22/22 [==============================] - 1s 34ms/step - loss: 0.1642 - accuracy: 0.9593 - val_loss: 0.0639 - val_accuracy: 1.0000\n",
            "Epoch 39/50\n",
            "22/22 [==============================] - 1s 36ms/step - loss: 0.2352 - accuracy: 0.9477 - val_loss: 0.0695 - val_accuracy: 1.0000\n",
            "Epoch 40/50\n",
            "22/22 [==============================] - 1s 39ms/step - loss: 0.2700 - accuracy: 0.9419 - val_loss: 0.0725 - val_accuracy: 1.0000\n",
            "Epoch 41/50\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 0.3233 - accuracy: 0.9012 - val_loss: 0.0692 - val_accuracy: 1.0000\n",
            "Epoch 42/50\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 0.2794 - accuracy: 0.9360 - val_loss: 0.0638 - val_accuracy: 1.0000\n",
            "Epoch 43/50\n",
            "22/22 [==============================] - 1s 37ms/step - loss: 0.1944 - accuracy: 0.9432 - val_loss: 0.0676 - val_accuracy: 1.0000\n",
            "Epoch 44/50\n",
            "22/22 [==============================] - 1s 38ms/step - loss: 0.1442 - accuracy: 0.9659 - val_loss: 0.0574 - val_accuracy: 1.0000\n",
            "Epoch 45/50\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 0.1678 - accuracy: 0.9535 - val_loss: 0.0561 - val_accuracy: 1.0000\n",
            "Epoch 46/50\n",
            "22/22 [==============================] - 1s 36ms/step - loss: 0.1238 - accuracy: 0.9709 - val_loss: 0.0657 - val_accuracy: 1.0000\n",
            "Epoch 47/50\n",
            "22/22 [==============================] - 1s 38ms/step - loss: 0.1836 - accuracy: 0.9651 - val_loss: 0.0796 - val_accuracy: 1.0000\n",
            "Epoch 48/50\n",
            "22/22 [==============================] - 1s 31ms/step - loss: 0.1684 - accuracy: 0.9593 - val_loss: 0.0527 - val_accuracy: 1.0000\n",
            "Epoch 49/50\n",
            "22/22 [==============================] - 1s 36ms/step - loss: 0.1343 - accuracy: 0.9709 - val_loss: 0.0452 - val_accuracy: 1.0000\n",
            "Epoch 50/50\n",
            "22/22 [==============================] - 1s 34ms/step - loss: 0.1786 - accuracy: 0.9477 - val_loss: 0.0463 - val_accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5QNaE1i-meg",
        "outputId": "2df717d9-87fe-4c24-95de-f90a8359ec7e"
      },
      "source": [
        "# evaluate the network\n",
        "print(\"[INFO] evaluating network...\")\n",
        "predictions = model.predict(x=testX, batch_size=BS)\n",
        "print(classification_report(testY.argmax(axis=1),\n",
        "\tpredictions.argmax(axis=1), target_names=le.classes_))\n",
        "# save the network to disk\n",
        "print(\"[INFO] serializing network to '{}'...\".format('/content/sample_data'))\n",
        "model.save('/content/sample_data/myPBmodel')\n",
        "#model.save('/content/sample_data/file1', save_format=\"h5\")\n",
        "# save the label encoder to disk\n",
        "f = open('/content/sample_data/myleEncoder', \"wb\")\n",
        "f.write(pickle.dumps(le))\n",
        "f.close()\n",
        "# plot the training loss and accuracy\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, EPOCHS), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, EPOCHS), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, EPOCHS), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, EPOCHS), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.savefig('/content/sample_data')"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        fake       1.00      1.00      1.00        22\n",
            "        real       1.00      1.00      1.00        38\n",
            "\n",
            "    accuracy                           1.00        60\n",
            "   macro avg       1.00      1.00      1.00        60\n",
            "weighted avg       1.00      1.00      1.00        60\n",
            "\n",
            "[INFO] serializing network to '/content/sample_data'...\n",
            "INFO:tensorflow:Assets written to: /content/sample_data/myPBmodel/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vMoc4IrDcAp"
      },
      "source": [
        "# import the necessary packages\n",
        "from imutils.video import VideoStream\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import argparse\n",
        "import imutils\n",
        "import pickle\n",
        "import time\n",
        "import cv2\n",
        "import os"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZ6s5TlaEIbE"
      },
      "source": [
        "# load our serialized face detector from disk\n",
        "print(\"[INFO] loading face detector...\")\n",
        "#protoPath = os.path.sep.join([args[\"detector\"], \"deploy.prototxt\"])\n",
        "#modelPath = os.path.sep.join([args[\"detector\"],\"res10_300x300_ssd_iter_140000.caffemodel\"])\n",
        "net = cv2.dnn.readNetFromCaffe('/content/sample_data/deploy.prototxt.txt', '/content/sample_data/openCVfaceDetectorModel1.caffemodel')\n",
        "# load the liveness detector model and label encoder from disk\n",
        "print(\"[INFO] loading liveness detector...\")\n",
        "model = load_model('/content/sample_data/model')\n",
        "le = pickle.loads(open('/content/sample_data/leEncoder', \"rb\").read())\n",
        "# initialize the video stream and allow the camera sensor to warmup\n",
        "print(\"[INFO] starting video stream...\")\n",
        "vs = VideoStream(src=0).start()\n",
        "time.sleep(2.0)\n",
        "\n",
        "# loop over the frames from the video stream\n",
        "while True:\n",
        "\t# grab the frame from the threaded video stream and resize it\n",
        "\t# to have a maximum width of 600 pixels\n",
        "\tframe = vs.read()\n",
        "\tframe = imutils.resize(frame, width=600)\n",
        "\t# grab the frame dimensions and convert it to a blob\n",
        "\t(h, w) = frame.shape[:2]\n",
        "\tblob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0,\n",
        "\t\t(300, 300), (104.0, 177.0, 123.0))\n",
        "\t# pass the blob through the network and obtain the detections and\n",
        "\t# predictions\n",
        "\tnet.setInput(blob)\n",
        "\tdetections = net.forward()\n",
        "  # loop over the detections\n",
        "\tfor i in range(0, detections.shape[2]):\n",
        "\t\t# extract the confidence (i.e., probability) associated with the\n",
        "\t\t# prediction\n",
        "\t\tconfidence = detections[0, 0, i, 2]\n",
        "\t\t# filter out weak detections\n",
        "\t\tif confidence > args[\"confidence\"]:\n",
        "\t\t\t# compute the (x, y)-coordinates of the bounding box for\n",
        "\t\t\t# the face and extract the face ROI\n",
        "\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\t\t\t# ensure the detected bounding box does fall outside the\n",
        "\t\t\t# dimensions of the frame\n",
        "\t\t\tstartX = max(0, startX)\n",
        "\t\t\tstartY = max(0, startY)\n",
        "\t\t\tendX = min(w, endX)\n",
        "\t\t\tendY = min(h, endY)\n",
        "\t\t\t# extract the face ROI and then preproces it in the exact\n",
        "\t\t\t# same manner as our training data\n",
        "\t\t\tface = frame[startY:endY, startX:endX]\n",
        "\t\t\tface = cv2.resize(face, (32, 32))\n",
        "\t\t\tface = face.astype(\"float\") / 255.0\n",
        "\t\t\tface = img_to_array(face)\n",
        "\t\t\tface = np.expand_dims(face, axis=0)\n",
        "\t\t\t# pass the face ROI through the trained liveness detector\n",
        "\t\t\t# model to determine if the face is \"real\" or \"fake\"\n",
        "\t\t\tpreds = model.predict(face)[0]\n",
        "\t\t\tj = np.argmax(preds)\n",
        "\t\t\tlabel = le.classes_[j]\n",
        "\t\t\t# draw the label and bounding box on the frame\n",
        "\t\t\tlabel = \"{}: {:.4f}\".format(label, preds[j])\n",
        "\t\t\tcv2.putText(frame, label, (startX, startY - 10),\n",
        "\t\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
        "\t\t\tcv2.rectangle(frame, (startX, startY), (endX, endY),\n",
        "\t\t\t\t(0, 0, 255), 2)\n",
        "   # show the output frame and wait for a key press\n",
        "\tcv2.imshow(\"Frame\", frame)\n",
        "\tkey = cv2.waitKey(1) & 0xFF\n",
        "\t# if the `q` key was pressed, break from the loop\n",
        "\tif key == ord(\"q\"):\n",
        "\t\tbreak\n",
        "# do a bit of cleanup\n",
        "cv2.destroyAllWindows()\n",
        "vs.stop()\n",
        "      \n",
        "      \n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDdMus4XIcwQ",
        "outputId": "e1fa44bd-edb9-4b9b-d6d1-0a36b036ae71"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Convert the model\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model('/content/sample_data/myPBmodel') # path to the SavedModel directory\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model.\n",
        "with open('model.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ]
    }
  ]
}